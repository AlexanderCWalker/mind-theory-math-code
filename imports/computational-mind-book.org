#+Title: The Routledge Handbook of the Computational Mind
#+Author: Britt Anderson

* Chapter 8 Classic Computational Models (CCM)
:PROPERTIES:
:chapter-author: Richard Samuels
:END:

** Core Characteristics
*** Symbolic
Symbols are /representations/ (that is they can stand-in and take the place of that which they are said to represent) and belong to a /system/. Think language. A system not only specifies the dictionary of what symbols are permissible, but what is their definition. They also provide the rules for their usage; think grammar.

:class_question:
What in the computer language that you are working with matches this symbol component of the CCM? Is it a /natural/ component of your language?
:END:

*** Algorithmic
While the above symbol grammar describes the legality of symbol constructions, the algorithmic rules specify additional rules of combinations: how to transform inputs to outputs. Think mappings.

:class_question:
Can you think of a good mathematical metaphor or example for this /algorithmic/ aspect of CCMs?
:END:

*** Formal
Algorithms are understood in terms of syntax not semantics. Symbol manipulations follows rules that are mediated by the rules of symbol transformation, not the meaning of what it is the symbols are supposed to represent. Think formal logic. Think the rules of multiplication. These only depend on the symbols representing numbers not whether a numeric symbol means 3 or 5. 

*** Interpretable
Although all the above rests of formal symbol manipulation, the symbols do have semantic associations and we should be able to interpret the results semantically. 
:class_question:
Human memory is finite. The tape of a Turing machine is infinite. Does that mean that human memory cannot be classically computationally modeled?
:END:

:class_question:
How does this characterization of the classical computation model match up with [[http://hdl.handle.net/1721.1/5782][Marr's levels]]? How many levels did Marr propose?
:END:

** Virtues
1. Lead to testable predictions.
2. May be useful even if they are conceptually wrong (that is the mind is not a CCM). For example, electrical circuit theory may be useful for modeling neurons even if neurons are not /just/ electrical circuits.
3. Avoids underspecification.
4. Avoids vacuity.
5. Address "how" questions.
6. Address "how-possibly" questions.
7. Handle _productivity_ and _systematicity_.
8. Seems to fit well the need for variables and quantifications.
** Challenges
*** A priori philosophical objections.
Searle's Chinese Room argument says that even when the "right" computations are performed, the predicted cognitive consequences do not appear, therefore CCM's are insufficient for cognition.
:class_question:
How does this map onto the hierarchy of [[*Core Characteristics][core characteristics]] specified above?

We are looking to touch on the adherence to all the right syntactic rules without the emergence of semantic content. The man can translate Chinese without speaking Chinese. 
:END:
*** Mathematical Objections
This is generally some form of GÃ¶del's Incompleteness results (see for a famous example [[https://en.wikipedia.org/wiki/The_Emperor%27s_New_Mind][Penrose's book]]).
*** Explanatory Lack
* Chapter 20 Representation
** What are representations?
   Egan suggests a key aspect of modeling comes down to selecting a representation.

   :class_question:    Does a pattern of neuronal firing that is statistically associated with some external stimulus or some internal feeling "represent" that stimulus or that feeling? Can neurons ever be said to "represent?"

   There is distinction between /vehicle/ and /content/.

   In the deductive-nomological ideas of scientific explanation (to come) there is the deductive component that is determinate, and the nomological that gives the character of a law that provides the basis for a certain thing being a certain way. It seems like assertions about representations are in a sense their own mini-theory for minds.

   :class_question: why should representations allow for the possibility of "/getting it wrong/."

   Naturalism and mechanism are other common aspects. The idea is that we don't want to let belief somehow sneak into our definition of what belief is. We want to avoid question begging, and naturalism allows that.

   I have a little trouble with what a tracking theory is, but I think it is basically the idea that your belief about whether something is or is not the case  corresponds to when it is, or is not, actually the case. Thus, if your mental states "track" some neuronal firing pattern then we can talk about those neurons' firings being representations of your mental state.

   :class_question: what separates the information-theoretic theories from the tracking theories? Can you explain the problem Egan outlines for this family of theories?

   Structural similarity theories can be suggested by the idea that a map (a cartographic object) represents some location. They are similar. You can relate clearly and unambiguously changes in map locations to world locations. This talks of maps, structure, domains and so on will come up again in category theory. That would seem a productive language for this talk of structural similarity representation.

   :class_question: what is the critique behind "isomorphisms are cheap"?


   :class_question: In light of Egan's argument to this point do we agree or disagree with the claim that place cells in the hippocampus represent locations?

   I found intriguing her observation that "computational theorists ... typically look to an organism's behavior and to the environment in which the behavior is normally deployed when they assign representational content ..." Doesn't that ignore the whole level psychologists are interested in, and what actually makes all this hard? That is accounting for feelings and subjectivity? The stuff that psychology was invented for in the first place? Thus, perhaps we should have some sympathy towards the phenomenal intentionalists?

   Noting all these problems Egan develops a "Deflationary Account of Content." The first component is the "function theoretic", e.g. an internal integrator to track eye position or movement. To explain something you appeal to, or even identify it with, a well characterized mathematical function. It separates ("prescinds") the mechanism from both the enviroment and the cognitive capacity. The content is determined by the mathematical function.

   This leads to her four components i) the FT ii) the algorithm for computing it iii) the representational structures the algorithms maintains and iv) the computational processes carried out over these structures. And a v)th component that is the ecological constraint about why this computation is useful in this environment. 

   :class_question: any early ideas how this maps on to our thinking about math and code and particular computer languages?

   :class_question: what is a [[https://en.wikipedia.org/wiki/Homomorphism][homomorphism]]? Egan mentions it.

   :class_question: doesn't this leave out the cognitive part of a computational cognitive theory? How is this accounted for by the /intentional gloss/.

   "Explanatory purpose resolves indeterminacy." This sentence might be one of the key insights.

   :class_question: what makes the theory deflationary?

many modelers and neurobiologists talk about patterns of neuronal firing representing some aspect of the world. Often the practical intent is to signal that there is a correlation between the neuronal pattern on the one hand and the presence of the world content on the other. However, that is not what is meant by representation in the philosophical or theoretical sense. This chapter by William Ramsey talks a lot about what a /structural/ or S representation is and what the challenges are to them. A good chapter if class discussion begins to get hung up on this point.
* Chapter 21 Computational Explanations and Neural Coding
Emphasizes that for neurons to represent things the neuronal pattern must play a role in the system separate from what it signals to us as observers.
** Principles
- Correlation :: certain kinds of content associated to certain neural patterns
- Proportionality :: increased neural strength : increased signal strength
- Decodability :: very important here to distinguish what the system does from what the scientist running the experiment does.
** The Critique of Divisive Normalization (Canonical Computations)
Argues that the Heeger and Caradini claims are misleading. This is in fact a model of neural processes that does not in any way sanction a discussion of semantic content. Picking the example of saturating V1 responses with increasing stimulus contrast and independent of stimulus orientation, the chapter author (Cao) argues this "describes" the neural relations between input stimulus contrast and neuronal firing patterns. She also argues that this is all in the service of preserving the signal, however that is a content-free process and thus divorced from cognition which is all about transformation of the signal. Models of cognition are not about /reproduction/, but about /transformation/. Divisive normalization optimizes the non-semantic features of a signal. She has a nice section starting on p 291 for what an honest model would look like. 
  

