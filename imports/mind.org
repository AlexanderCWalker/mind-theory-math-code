#+Title: *MIND*-theory-math-code

* Prologue
We are interested in making models of the brain and mind. We would like the results of our model making to move us to make better inferences; to produce better explanations, but we cannot make a good model of something we do not understand. So, we begin by asking the question: what is cognition?

* Plan



To do this we need the best tools. The tools of our yesterday may not be the tools we need today. One can build computers out of vacumn tubes, but it is better to build them out of integrated circuits. The tools of the model builder are math and code.

One of the goals of this course is to explore areas of math that have been under-represented in our training. Are there new, at least to us, domains of mathematics that will offer us advantages in our modeling work?

A second goal is to critically think about the programming languages we use to express our models. There is a temptation to think that Turing completeness makes our choice of programming language largely irrelevant. To be partly accident, partly personal preference. But which of us would want to code our model in machine code? Certainly some things are easier to do in some languages than others. But more importantly, models are what they express. What sorts of content are easier to express in which  programming language? Is it possible that our choice of programming language influences the sorts of models we think to create? How does this interact with the mathematical content of our model. We will want to braid our examination of new math with our programming language choices.

But there is stage before this that we rarely explicitly consider. And when we do it is usually at an abstract level. Not in reference to a specific research question, nor the affordances of our math and programming language choices. /Is our research question a meaningful one?/

Imagine that I provide you a file of eye movements recorded from people looking at a computer display in some psychology experiment. Or that you are given the data of spike times from multiple neurons recorded in an awake behaving primate solving some task. I then ask you if your model fits these data. You go away, write your code, run your model, compare your results, and say yes, no, or, more likely, somewhat and sometimes. Was that effort /scientifically/ useful? What are the criteria that we should use to judge scientific utility? How much of the answer depends on the framing of the original question that led to the experimental data being collected in the first place? How much hinges on the effort to fit those data to a model? Are there important considerations to be undertaken before the experiment and before the modeling if we want to be engaged in a truly /scientific/ project?

I am in search of antecedents. Does how we frame our particular experiment or our particular model importantly impact on achieving our goal of having our models matter? Are there specific details of the framing that are particular to cognitive and neuroscientific applications? Therefore, before we move on to the math and the code, I want to reflect what we should think about when developing our ideas about how things work and whether it is necessary for them to have clear theoretical implications?

* Discussion Questions

  1. What does it mean for something to be a cognitive model?
     - Is it enough to say it in words?

     - Are there constraints on the language we use to express a cognitive model that makes it a useful construct?
       
     - Does our model have to be expressed as a computer program? Or is that merely practically useful? Does our understanding of the model, or our ability to evalute it as a scientific proposal, benefit from the translation into code?

  2. Do our models need to explain?

     - What is scientific explanation and how does considering explanation impact what we regard as a useful model?

  3. What is a theory?

     - Do our ideas and models about how things work need to be expressible in some particular form to be designated a "theory"?

     - Does expressing them as a theory improve our ability to test our ideas? Is it worth the effort?

  4. Are we doing a good job of making sure that our ideas about cognition and our investigations comparing empirical and computer simulations address the theoretical implications?

  5. What are we commiting ourselves to when we say we have a "computational" model?

     - What does it mean for something to be computational?

     - What is the computational theory of mind?

     - Is this theory consistent what we think we are doing?

  6. Does this theoretical position hold implications for the necessity or even just the relevance of neural data for cognitive theories?

     - Do cognitive models need to make predictions about neural implementations?

  7. Are these considerations less relevant for computational neuroscience?

     - Do computational neuroscience models need to have implications for cognition?

* Readings

The first two items are articles. The last few are entries from the Stanford Encyclopedia that give context and background. As a group the readings are quite long and I don't think it makes sense to sit down and try to read each carefully. My recommendation is to skim the Stanford Encyclopedia entries to get a flavor for the variety, breadth, and terminology, but don't worry about the things you don't understand. Then read the Suppes article more carefully looking back to the Stanford Encyclopedia article to clarify any missing terms. I have a [[file:mind-suppes.org][list of questions]] that we can use to guide a discussion of this article. The article [[*\[\[https://www.jstor.org/stable/pdf/2393788.pdf?refreqid=excelsior%3Ac41e16772ba91564597e9aafaa59c1a9\]\[What Theory is Not\]\] (pdf)][What Theory is Not (pdf)]] is a nice application of the above. It looks at practice in the field or organizational psychology, but its critiques are pertinent to computational neuroscience too. We might want to see how we think our current favorite articles measure up against these ideas. 

The last two articles offer a prescription for psychology (and I would argue for neuroscience and the computational versions too). The real question is whether you agree? Do they address a problem. Is it the problem that Suppes and others note? Is it consistent with the uses of "theory" in the philosophical sense of the word? And if all this bothers you, feels too constraining, then check out Feyerabend's /Against Method./

** [[https://suppes-corpus.stanford.edu/sites/g/files/sbiybj7316/f/what_is_a_scientific_theory_84.pdf][What is a scientific theory]] (pdf).
   
*** Citation
    @inbook {253,
	title = {What is a scientific theory?},
	booktitle = {Philosophy of Science Today},
	year = {1967},
	pages = {55-67},
	publisher = {Basic Books},
	organization = {Basic Books},
	chapter = {6},
	address = {New York},
	keywords = {Methodology and Probability and Measurement},
	author = {Suppes, Patrick},
	editor = {Morgenbesser, S.}
}


** [[https://www.jstor.org/stable/pdf/2393788.pdf?refreqid=excelsior%3Ac41e16772ba91564597e9aafaa59c1a9][What Theory is Not]] (pdf)

*** Citation
@article{10.2307/2393788,
 ISSN = {00018392},
 URL = {http://www.jstor.org/stable/2393788},
 abstract = {This essay describes differences between papers that contain some theory rather than no theory. There is little agreement about what constitutes strong versus weak theory in the social sciences, but there is more consensus that references, data, variables, diagrams, and hypotheses are not theory. Despite this consensus, however, authors routinely use these five elements in lieu of theory. We explain how each of these five elements can be confused with theory and how to avoid such confusion. By making this consensus explicit, we hope to help authors avoid some of the most common and easily averted problems that lead readers to view papers as having inadequate theory. We then discuss how journals might facilitate the publication of stronger theory. We suggest that if the field is serious about producing stronger theory, journals need to reconsider their empirical requirements. We argue that journals ought to be more receptive to papers that test part rather than all of a theory and use illustrative rather than definitive data.},
 author = {Robert I. Sutton and Barry M. Staw},
 journal = {Administrative Science Quarterly},
 number = {3},
 pages = {371--384},
 publisher = {[Sage Publications, Inc., Johnson Graduate School of Management, Cornell University]},
 title = {What Theory is Not},
 volume = {40},
 year = {1995}
}

** [[https://plato.stanford.edu/entries/structure-scientific-theories/#SynSemPraVieBas][The Structure of Scientific Theories]] (not Kuhn)

*** Citation
@InCollection{sep-structure-scientific-theories,
	author       =	{Winther, Rasmus Gr√∏nfeldt},
	title        =	{{The Structure of Scientific Theories}},
	booktitle    =	{The {Stanford} Encyclopedia of Philosophy},
	editor       =	{Edward N. Zalta},
	howpublished =	{\url{https://plato.stanford.edu/archives/spr2021/entries/structure-scientific-theories/}},
	year         =	{2021},
	edition      =	{{S}pring 2021},
	publisher    =	{Metaphysics Research Lab, Stanford University}
}

** [[https://plato.stanford.edu/entries/scientific-explanation/][Scientific Explanation]]

*** Citation
@InCollection{sep-scientific-explanation,
	author       =	{Woodward, James and Ross, Lauren},
	title        =	{{Scientific Explanation}},
	booktitle    =	{The {Stanford} Encyclopedia of Philosophy},
	editor       =	{Edward N. Zalta},
	howpublished =	{\url{https://plato.stanford.edu/archives/sum2021/entries/scientific-explanation/}},
	year         =	{2021},
	edition      =	{{S}ummer 2021},
	publisher    =	{Metaphysics Research Lab, Stanford University}
}

**  [[https://plato.stanford.edu/entries/model-theory/][Model Theory]]

*** Citation
@InCollection{sep-model-theory,
	author       =	{Hodges, Wilfrid},
	title        =	{{Model Theory}},
	booktitle    =	{The {Stanford} Encyclopedia of Philosophy},
	editor       =	{Edward N. Zalta},
	howpublished =	{\url{https://plato.stanford.edu/archives/win2020/entries/model-theory/}},
	year         =	{2020},
	edition      =	{{W}inter 2020},
	publisher    =	{Metaphysics Research Lab, Stanford University}
}

** [[https://doi.org/10.1177%2F1745691620970585][How Computational Modeling Can Force Theory Building in Psychological Science]]

*** Citation
@article{guest21_how_comput_model_can_force,
  author =	 {Olivia Guest and Andrea E. Martin},
  title =	 {How Computational Modeling Can Force Theory Building
                  in Psychological Science},
  journal =	 {Perspectives on Psychological Science},
  volume =	 {nil},
  number =	 {nil},
  pages =	 174569162097058,
  year =	 2021,
  doi =		 {10.1177/1745691620970585},
  url =		 {https://doi.org/10.1177/1745691620970585},
  DATE_ADDED =	 {Sun Jun 20 13:02:36 2021},
}
** [[https://doi.org/10.1177/1745691620970604][Theory Before the Test: How to Build High-Verisimilitude Explanatory Theories in Psychological Science]]

*** Citation
@article{rooij21_theor_befor_test,
  author =	 {Iris van Rooij and Giosu{\`e} Baggio},
  title =	 {Theory Before the Test: How To Build
                  High-Verisimilitude Explanatory Theories in
                  Psychological Science},
  journal =	 {Perspectives on Psychological Science},
  volume =	 {nil},
  number =	 {nil},
  pages =	 174569162097060,
  year =	 2021,
  doi =		 {10.1177/1745691620970604},
  url =		 {https://doi.org/10.1177/1745691620970604},
  DATE_ADDED =	 {Sun Jun 20 13:08:18 2021},
}



